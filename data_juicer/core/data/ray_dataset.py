from __future__ import annotations

from collections import defaultdict
from concurrent import futures
import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
from argparse import Namespace
from functools import partial
import subprocess
import time
from typing import Any, Dict, List, Literal, Optional, Union
import uuid
os.environ["RAY_DEDUP_LOGS"] = "1"
from data_juicer.core.ray_actor import ComputeActor, filter_batch
import pyarrow
import pyarrow as pa    
import ray.data as rd
import ray.data.read_api as ds
from loguru import logger

from data_juicer import cuda_device_count
from data_juicer.core.data import DJDataset
from data_juicer.core.data.schema import Schema
from data_juicer.ops import Deduplicator, Filter, Mapper
from data_juicer.ops.base_op import TAGGING_OPS
from data_juicer.utils.constant import Fields
from data_juicer.utils.process_utils import calculate_np

import ray
from ray.util.placement_group import (
    placement_group,
    placement_group_table,
    remove_placement_group,
)
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy


def get_abs_path(path, dataset_dir):
    full_path = os.path.abspath(os.path.join(dataset_dir, path))
    if os.path.exists(full_path):
        return full_path
    else:
        return path


def convert_to_absolute_paths(samples, dataset_dir, path_keys):
    samples = samples.to_pydict()
    for key in path_keys:
        for idx in range(len(samples[key])):
            paths = samples[key][idx]
            if isinstance(paths, str):
                samples[key][idx] = get_abs_path(paths, dataset_dir)
            elif isinstance(paths, list):
                samples[key][idx] = [
                    get_abs_path(item, dataset_dir) for item in paths
                ]
    return pyarrow.Table.from_pydict(samples)


# TODO: check path for nestdataset
def set_dataset_to_absolute_path(dataset, dataset_path, cfg):
    """
    Set all the path in input data to absolute path.
    Checks dataset_dir and project_dir for valid paths.
    """
    path_keys = []
    columns = dataset.columns()
    for key in [cfg.video_key, cfg.image_key, cfg.audio_key]:
        if key in columns:
            path_keys.append(key)
    if len(path_keys) > 0:
        dataset_dir = os.path.dirname(dataset_path)
        logger.error(f'dataset_dir: {dataset_dir}')
        dataset = dataset.map_batches(partial(convert_to_absolute_paths,
                                              dataset_dir=dataset_dir,
                                              path_keys=path_keys),
                                      batch_format='pyarrow',
                                      zero_copy_batch=True)
    return dataset


def preprocess_dataset(dataset: rd.Dataset, dataset_path, cfg) -> rd.Dataset:
    if dataset_path:
        dataset = set_dataset_to_absolute_path(dataset, dataset_path, cfg)
    return dataset


def get_num_gpus(op, op_proc):
    if not op.use_cuda():
        return 0
    proc_per_gpu = op_proc / cuda_device_count()
    return 1.0 / proc_per_gpu


# def filter_batch(batch, filter_func):
#     mask = pyarrow.array(filter_func(batch.to_pydict()))
#     return batch.filter(mask)

class MPSManager:
    """MPSèµ„æºç®¡ç†å™¨"""
    
    def __init__(self, target_gpu_id="0"):
        self.target_gpu_id = target_gpu_id
        self.current_percentage = 100
        self.op_allocations = {
            'video_aesthetics_filter': 50,  
            # 'video_captioning_from_frames_mapper': 10,  
        }  # å­˜å‚¨æ¯ä¸ªopçš„èµ„æºåˆ†é…
        
    def init_mps(self):
        """åˆå§‹åŒ– MPS æœåŠ¡å™¨ï¼Œç¡®ä¿å®Œå…¨æ¸…ç†æ—§çŠ¶æ€"""
        try:
            subprocess.run(["pkill", "-9", "nvidia-cuda-mps"], check=False)
            subprocess.run(["pkill", "-9", "nvidia-cuda-mps-control"], check=False)
            time.sleep(1)  # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢


            subprocess.run(
                ["nvidia-smi", "-i", str(self.target_gpu_id), "--compute-mode=0"],
                check=True,
            )


            result = subprocess.run(
                ["nvidia-smi", "-i", str(self.target_gpu_id), "-q", "-d", "PIDS"],
                capture_output=True,
                text=True,
            )
            if 'MPS Server' in result.stdout:
                logger.info(f"MPS Server already running on GPU {self.target_gpu_id}")
            else:
                logger.info(f"Starting MPS Server on GPU {self.target_gpu_id}")
                subprocess.run(['nvidia-cuda-mps-control', '-d'], check=True)
                logger.info(f"MPS Server started on GPU {self.target_gpu_id}")


            self.set_mps_percentage(100)
            logger.info(f"MPS initialized on GPU {self.target_gpu_id}")

        except Exception as e:
            logger.error(f"Failed to initialize MPS: {e}")
            raise
    
    def set_mps_percentage(self, percentage):
        """è®¾ç½®MPSæ´»è·ƒçº¿ç¨‹ç™¾åˆ†æ¯”"""
        try:
            # è®¾ç½®ç‹¬å æ¨¡å¼
            subprocess.run(['nvidia-smi', '-i', str(self.target_gpu_id), 
                        '--compute-mode=EXCLUSIVE_PROCESS'], check=True)
            
            # ç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡å¹¶ç¡®ä¿åç»­è¿›ç¨‹ç»§æ‰¿
            os.environ['CUDA_VISIBLE_DEVICES'] = str(self.target_gpu_id)
            os.environ['CUDA_MPS_ACTIVE_THREAD_PERCENTAGE'] = str(percentage)
            
            # æ˜¾å¼å¯åŠ¨ä¸€ä¸ªè®¾ç½®ç™¾åˆ†æ¯”çš„MPSæ§åˆ¶è¿›ç¨‹
            subprocess.run(
                f'echo "set_default_active_thread_percentage {percentage}" | nvidia-cuda-mps-control',
                shell=True,
                check=True
            )
            
            self.current_percentage = percentage
            logger.info(f"Set MPS percentage to {percentage}% on GPU {self.target_gpu_id}")
            
        except Exception as e:
            logger.warning(f"Failed to set MPS percentage: {e}")
            raise
    
    def allocate_resources_for_ops(self, operators):
        """ä¸ºæ“ä½œç¬¦åˆ†é…MPSèµ„æº"""
        if not operators:
            return {}
        
        # æ£€æŸ¥æ¯ä¸ªopçš„åˆ†é…æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        for op in operators:
            if op._name in self.op_allocations:
                allocation = self.op_allocations[op._name]
                if not isinstance(allocation, int) or allocation < 0 or allocation > 100:
                    logger.warning(f"Invalid allocation for op {op._name}: {allocation}. "
                                   f"Allocations should be integers between 0 and 100.")
                    return {}
            else:
                logger.warning(f"No allocation set for op {op._name}.")
        
        logger.info(f"MPS resource allocation: {self.op_allocations}")
        return self.op_allocations
    
    def get_runtime_env_for_op(self, op_name, base_env=None):
        """è·å–ç‰¹å®šæ“ä½œç¬¦çš„è¿è¡Œæ—¶ç¯å¢ƒ"""
        runtime_env = base_env or {}
        
        if 'env_vars' not in runtime_env:
            runtime_env['env_vars'] = {}
        
        runtime_env['env_vars']['CUDA_VISIBLE_DEVICES'] = str(self.target_gpu_id)
        
        # è®¾ç½®è¯¥opçš„MPSç™¾åˆ†æ¯”
        percentage = self.op_allocations.get(op_name, 50)  # é»˜è®¤50%
        runtime_env['env_vars']['CUDA_MPS_ACTIVE_THREAD_PERCENTAGE'] = str(percentage)
        
        # æ·»åŠ Nsighté…ç½®
        # if 'nsight' not in runtime_env:
        #     runtime_env['nsight'] = {
        #         "t": "cuda,nvtx,osrt",
        #         "cuda-memory-usage": "true",
        #         "stats": "true",
        #         "cuda-graph-trace": "graph",
        #     }
        
        return runtime_env


class RayDataset(DJDataset):

    def __init__(self,
                dataset: rd.Dataset,
                dataset_path: str = None,
                cfg: Optional[Namespace] = None) -> None:
        self.data = preprocess_dataset(dataset, dataset_path, cfg)
        self.num_proc = getattr(cfg, 'np', getattr(cfg, 'num_proc', None)) if cfg else None
        self.gpu_pg = placement_group([{"CPU": 16, "GPU": 2}], strategy="STRICT_SPREAD")
        ray.get(self.gpu_pg.ready())
        
        # åˆå§‹åŒ–MPSç®¡ç†å™¨
        # self.mps_manager = MPSManager()
        # self.mps_manager.init_mps()

    def schema(self) -> Schema:
        """Get dataset schema.

        Returns:
            Schema: Dataset schema containing column names and types
        """
        if self.data is None or self.data.columns() is None:
            raise ValueError('Dataset is empty or not initialized')

        # Get schema from Ray dataset
        _schema = self.data.schema()

        # convert schema to proper list and dict
        column_types = {
            k: Schema.map_ray_type_to_python(v)
            for k, v in zip(_schema.names, _schema.types)
        }
        return Schema(column_types=column_types, columns=column_types.keys())

    def get(self, k: int) -> List[Dict[str, Any]]:
        """Get k rows from the dataset."""
        if k < 0:
            raise ValueError(f'k must be non-negative, got {k}')

        if k == 0:
            return []

        k = min(k, self.data.count())
        return list(self.data.limit(k).take())

    def get_column(self, column: str, k: Optional[int] = None) -> List[Any]:
        """Get column values from Ray dataset.

        Args:
            column: Name of the column to retrieve
            k: Optional number of rows to return. If None, returns all rows

        Returns:
            List of values from the specified column

        Raises:
            KeyError: If column doesn't exist
            ValueError: If k is negative
        """
        if (self.data is None or self.data.columns() is None
                or column not in self.data.columns()):
            raise KeyError(f"Column '{column}' not found in dataset")

        if k is not None:
            if k < 0:
                raise ValueError(f'k must be non-negative, got {k}')
            if k == 0:
                return []
            k = min(k, self.data.count())
            return [row[column] for row in self.data.limit(k).take()]

        return [row[column] for row in self.data.take()]
    
    # def process(self, operators, *, exporter=None, checkpointer=None, tracer=None) -> DJDataset:
    #     if operators is None:
    #         return self
    #     if not isinstance(operators, list):
    #         operators = [operators]
    #     for op in operators:
    #         self._run_single_op(op)
    #     return self
    def process(self, operators, *, exporter=None, checkpointer=None, tracer=None) -> DJDataset:
        if operators is None:
            return self
        if not isinstance(operators, list):
            operators = [operators]

        from ray.data import from_arrow

        actors = []

        # åˆå§‹åŒ–æ‰€æœ‰ actor
        for idx, op in enumerate(operators):
            columns = self.data.columns()
            if isinstance(op, Filter) and Fields.stats not in columns:
                def process_batch_arrow(table: pyarrow.Table):
                    new_column_data = [{} for _ in range(len(table))]
                    new_table = table.append_column(Fields.stats, [new_column_data])
                    return new_table
                self.data = self.data.map_batches(process_batch_arrow, batch_format='pyarrow')

            # batch_size = getattr(op, 'batch_size', 1) if op.is_batched_op() else 1
            # batch_size = 3
            if op.is_batched_op():
                batch_size = 10
            else:
                batch_size = 1

            gpu_allocate = 1 if op.use_cuda() else 0

            actor = ComputeActor.options(
                name=f"actor_{op._name}_{uuid.uuid4().hex[:4]}",
                # num_gpus=0.5 if op.use_cuda() else 0,
                num_gpus=gpu_allocate,
                num_cpus=4,
                scheduling_strategy=PlacementGroupSchedulingStrategy(
                    placement_group=self.gpu_pg,
                    placement_group_capture_child_tasks=True
                ),
                placement_group_bundle_index=idx,
                runtime_env={
                    "env_vars": {"CUDA_VISIBLE_DEVICES": "0,1"} if op.use_cuda() else {},
                    "nsight": {
                        "t": "cuda,nvtx,osrt",
                        "cuda-memory-usage": "true",
                        "stats": "true",
                        "cuda-graph-trace": "graph",
                    }
                }
            ).remote(op, batch_size)
            actors.append(actor)
        

        # å°†æ•°æ®æŒ‰ batch æ‹†åˆ†å¤„ç†ï¼Œå½¢æˆå¼‚æ­¥æµæ°´çº¿
        # input_batches = self.data.iter_batches(batch_size=batch_size, batch_format="pyarrow")
        input_batches = list(self.data.iter_batches(batch_size=batch_size, batch_format="pyarrow"))
        print(f"Total batches generated: {len(input_batches)}")
        # print(input_batches)
        # final result refs for each batch
        futures = []

        for i, batch in enumerate(input_batches):
            if all(op.is_batched_op() for op in operators):
                # å…¨ batched æ¨¡å¼ï¼šç›´æ¥é“¾å¼ä¼ è¾“
                ref = batch
                for op, actor in zip(operators, actors):
                    if isinstance(op, Filter):
                        ref = actor.process_and_filter.remote(ref)
                    elif isinstance(op, Mapper):
                        ref = actor.mapper.remote(ref)
                    else:
                        raise NotImplementedError(f"Unsupported op type: {type(op)}")
                futures.append(ref)
            
            else:
                # æ··åˆæ¨¡å¼
                current_refs = [batch]

                for op_idx, (op, actor) in enumerate(zip(operators, actors)):
                    new_refs = []

                    for ref in current_refs:
                        if op.is_batched_op():
                            if isinstance(ref, ray.ObjectRef):
                                ref = ray.get(ref)

                            if isinstance(ref, pyarrow.Table):
                                ref = ref.to_pylist()

                            new_ref = (
                                actor.process_and_filter.remote(ref) if isinstance(op, Filter)
                                else actor.mapper.remote(ref)
                            )
                            print(f"\nnew_ref: {new_ref}")
                            new_refs.append(new_ref)

                        else:
                            single_records = pyarrow.Table.to_pylist(ref)
                            for record in single_records:
                                single_table = pyarrow.Table.from_pylist([record])
                                ref_single = (
                                    actor.process_and_filter.remote(single_table)
                                    if isinstance(op, Filter)
                                    else actor.mapper_single.remote(single_table)
                                )
                                new_refs.append(ref_single)

                    # æ›´æ–° current_refs ç”¨äºä¸‹ä¸€è½®
                    current_refs = new_refs

                # âœ… æ‰€æœ‰ operators å¤„ç†å®Œä¹‹åå† appendï¼
                futures.append(current_refs)


        print(f"Number of futures: {len(futures)}")

        # Aggregate all results
        try:
            if futures:
                print("=" * 60)
                print("ğŸ” Step 1: Futures Submission Summary")
                print(f"â†’ Number of futures: {len(futures)}")
                print(futures)
                print("â†’ Operator chain:", [op.__class__.__name__ for op in operators])
                print("â†’ futures[0] type:", type(futures[0]))

                is_batched = all(op.is_batched_op() for op in operators)
                print(f"â†’ is_batched_op(): {is_batched}")

                flat_futures = (
                    [ref for sublist in futures for ref in sublist] if isinstance(futures[0], list) else futures
                )

                print("â†’ Total ObjectRefs to resolve:", len(flat_futures))

                results = ray.get(flat_futures)
                print("âœ… Step 3: Results Retrieved from ray.get()")
                print(f"â†’ Number of result items: {len(results)}")
                print(f"â†’ Sample result type: {type(results[0])}")

                if is_batched and isinstance(results[0], list):
                    all_batches = [item for sublist in results for item in sublist]
                    print(f"â†’ Flattened batched results: {len(all_batches)}")
                else:
                    all_batches = results

                # è¿‡æ»¤ç©ºç»“æœ
                all_batches = [b for b in all_batches if b is not None]
                print(f"â†’ Filtered non-null batches: {len(all_batches)}")
                for i, b in enumerate(all_batches[:3]):
                    if isinstance(b, pyarrow.Table):
                        print(f"  Batch[{i}] rows: {b.num_rows}, schema: {b.schema}")
                    else:
                        print(f"  Batch[{i}] is not pyarrow.Table â†’ {type(b)}")

                if all_batches:
                    # Step A: åˆå¹¶å¤šä¸ªè¡¨ä¸ºä¸€ä¸ªå¤§è¡¨ï¼ˆå¤šè¡Œï¼‰
                    merged_table = pyarrow.concat_tables(all_batches)
                    print("âœ… Step A: Tables Concatenated")
                    print(f"â†’ Merged rows: {merged_table.num_rows}")
                    print(f"â†’ Merged schema: {merged_table.schema}")

                    # Step B: è½¬æˆåˆ—è¡¨åˆå¹¶ç›¸åŒè§†é¢‘æ•°æ®ï¼ˆæŒ‰éœ€ï¼‰
                    records = merged_table.to_pylist()

                    # è°ƒç”¨æŒ‰è§†é¢‘èšåˆå‡½æ•°ï¼Œç¤ºä¾‹å†…å®šä¹‰
                    merged_records = self.merge_records_by_video(records)
                    print(f"âœ… Step B: Records merged by video, total {len(merged_records)} records")

                    # è½¬å› pyarrow.Table
                    final_table = pyarrow.Table.from_pylist(merged_records)

                    # try:
                    #     df = final_table.to_pandas()
                    #     print("â†’ Preview merged result:\n", df.head(5))
                    #     dup = df[df.duplicated()]
                    #     print(f"â†’ Duplicate rows detected: {len(dup)}")
                    # except Exception as e:
                    #     print(f"âš ï¸ Failed to convert to pandas: {e}")

                    self.data = from_arrow(final_table)
                else:
                    print("âš ï¸ No valid batch data after filtering.")
                    self.data = ray.data.from_items([])
            else:
                print("âš ï¸ No futures to process.")
                self.data = ray.data.from_items([])

        except Exception as e:
            import traceback
            print("Ray Task Error:", type(e), str(e))
            traceback.print_exc()
            raise e

        return self


    def merge_tables_to_one_row(self, all_batches: list[pa.Table]) -> pa.Table:
        # æŠŠæ¯ä¸ª Table è½¬ä¸º List[Dict]
        tables_as_dicts = [tbl.to_pylist() for tbl in all_batches]

        # æ„é€ å•æ¡è®°å½•ï¼Œkeyç”¨ mapper idx æ ‡è¯†
        merged_record = {
            f"mapper_{i+1}_results": tables_as_dicts[i]
            for i in range(len(tables_as_dicts))
        }

        # è½¬æˆåªæœ‰ä¸€æ¡è®°å½•çš„åˆ—è¡¨
        merged_list = [merged_record]

        # ç”Ÿæˆ pyarrow.Table è¿”å›
        return pa.Table.from_pylist(merged_list)
    
    def merge_records_by_video(self, records):
        merged = defaultdict(lambda: {
            "videos": None,
            "texts": [],
            "aesthetics_scores": []
        })

        for rec in records:
            # å…³é”®ç‚¹ï¼šç¡®ä¿ video_key æ˜¯ hashable ç±»å‹
            video_key = rec['videos']
            if isinstance(video_key, list):
                while isinstance(video_key, list):
                    video_key = video_key[0] if video_key else None
            video_key = str(video_key)

            if merged[video_key]["videos"] is None:
                merged[video_key]["videos"] = rec["videos"]

            merged[video_key]["texts"].append(rec.get("text"))
            merged[video_key]["aesthetics_scores"].append(rec.get("aesthetics_score"))

        merged_list = []
        for video_key, data in merged.items():
            merged_list.append({
                "videos": data["videos"],
                "texts": data["texts"],
                "aesthetics_scores": data["aesthetics_scores"]
            })
        return merged_list

    # def process(self, operators, *, exporter=None, checkpointer=None, tracer=None) -> DJDataset:
    #     if operators is None:
    #         return self
    #     if not isinstance(operators, list):
    #         operators = [operators]

    #     # é¢„å…ˆåˆ›å»ºæ‰€æœ‰operatorçš„actor
    #     actors = []
    #     for op in operators:
    #         columns = self.data.columns()
    #         if Fields.stats not in columns:

    #             def process_batch_arrow(table: pyarrow.Table):
    #                 new_column_data = [{} for _ in range(len(table))]
    #                 new_table = table.append_column(
    #                     Fields.stats, [new_column_data])
    #                 return new_table

    #             self.data = self.data.map_batches(process_batch_arrow,
    #                                                 batch_format='pyarrow')
    #         batch_size = getattr(op, 'batch_size', 1) if op.is_batched_op() else 1
    #         # ä¸ºæ¯ä¸ªoperatoråˆ›å»ºactor
    #         actor = ComputeActor.options(
    #             name=f"actor_{op._name}_{uuid.uuid4().hex[:4]}",
    #             num_gpus=0.5 if op.use_cuda() else 0,
    #             # num_gpus=0,
    #             scheduling_strategy=PlacementGroupSchedulingStrategy(
    #                 placement_group=self.gpu_pg,
    #                 placement_group_capture_child_tasks=True
    #             ),
    #             runtime_env={
    #                 "env_vars": {"CUDA_VISIBLE_DEVICES": "0,1"} if op.use_cuda() else {}
    #             }
    #         ).remote(op, batch_size)
    #         actors.append(actor)
        
    #     # åˆå§‹åŒ–æµæ°´çº¿
    #     current_data = self.data
    #     result_refs = []
        
    #     # å¯åŠ¨å¼‚æ­¥æµæ°´çº¿å¤„ç†
    #     for i, (op, actor) in enumerate(zip(operators, actors)):
    #         # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªoperatorï¼Œå¤„ç†åŸå§‹æ•°æ®
    #         if i == 0:
    #             input_data = current_data
    #         else:
    #             # å¦åˆ™ç­‰å¾…å‰ä¸€ä¸ªoperatorçš„ç»“æœ
    #             input_data = ray.get(result_refs[i-1])
            
    #         # æäº¤å½“å‰operatorçš„å¤„ç†ä»»åŠ¡
    #         ref = actor.process_and_filter.remote(input_data)
    #         result_refs.append(ref)
            
    #         # å¦‚æœæ˜¯æœ€åä¸€ä¸ªoperatorï¼Œç­‰å¾…ç»“æœå¹¶å¤„ç†è¾“å‡º
    #         if i == len(operators) - 1:
    #             final_result = ray.get(ref)
    #             if op.stats_export_path:
    #                 final_result.write_json(op.stats_export_path, force_ascii=False)
    #             self.data = final_result
        
    #     return self
    
    def _run_single_op(self, op):
        op_proc = calculate_np(op._name, op.mem_required, op.cpu_required,
                               self.num_proc, op.use_cuda())
        op_proc = 1
        num_gpus = get_num_gpus(op, op_proc)
        num_gpus = 0.5

        if (op._name in TAGGING_OPS.modules
                and Fields.meta not in self.data.columns()):

            def process_batch_arrow(table: pyarrow.Table):
                new_column_data = [{} for _ in range(len(table))]
                new_table = table.append_column(Fields.meta, [new_column_data])
                return new_table

            self.data = self.data.map_batches(process_batch_arrow,
                                              batch_format='pyarrow')

        try:
            batch_size = getattr(op, 'batch_size', 1) if op.is_batched_op() else 1
            
            # è·å–è¯¥æ“ä½œç¬¦çš„è¿è¡Œæ—¶ç¯å¢ƒé…ç½®
            runtime_env = self.mps_manager.get_runtime_env_for_op(op._name)
            
            if isinstance(op, Mapper):
                if op.use_cuda():
                    op_kwargs = op._op_cfg[op._name]
                    logger.info(f'Running Mapper {op._name} with MPS allocation: '
                              f'{self.mps_manager.op_allocations.get(op._name, "default")}%')
                    
                    self.data = self.data.map_batches(
                        op.__class__,
                        fn_args=None,
                        fn_kwargs=None,
                        fn_constructor_args=None,
                        fn_constructor_kwargs=op_kwargs,
                        batch_size=batch_size,
                        num_gpus=1,
                        concurrency=1,
                        batch_format='pyarrow',
                        scheduling_strategy=PlacementGroupSchedulingStrategy(
                            placement_group=self.gpu_pg,
                            placement_group_capture_child_tasks=True
                        ),
                        runtime_env=runtime_env)

                else:
                    self.data = self.data.map_batches(op.process,
                                                      batch_size=batch_size,
                                                      batch_format='pyarrow',
                                                      num_gpus=num_gpus)
                                                      
            elif isinstance(op, Filter):
                columns = self.data.columns()
                if Fields.stats not in columns:

                    def process_batch_arrow(table: pyarrow.Table):
                        new_column_data = [{} for _ in range(len(table))]
                        new_table = table.append_column(
                            Fields.stats, [new_column_data])
                        return new_table

                    self.data = self.data.map_batches(process_batch_arrow,
                                                      batch_format='pyarrow')
                                                      
                if op.use_cuda():
                    op_kwargs = op._op_cfg[op._name]
                    logger.info(f'Running Filter {op._name} with MPS allocation: '
                              f'{self.mps_manager.op_allocations.get(op._name, "default")}%')
                    
                    try:
                        # åˆ›å»ºActorï¼ˆèµ„æºå‚æ•°å…¨éƒ¨åœ¨optionsä¸­è®¾ç½®ï¼‰
                        actor = ComputeActor.options(
                            name=f"actor_{op._name}_{uuid.uuid4().hex[:4]}",  # å”¯ä¸€åç§°
                            num_gpus=0.5,      # å…±äº«GPU
                            scheduling_strategy=PlacementGroupSchedulingStrategy(
                                placement_group=self.gpu_pg,
                                placement_group_capture_child_tasks=True
                            ),
                            runtime_env={
                                "env_vars": {"CUDA_VISIBLE_DEVICES": "0"}  # æ˜ç¡®æŒ‡å®šGPU
                            }
                        ).remote(op, batch_size)

                        # æ‰§è¡Œå¹¶è·å–ç»“æœ
                        result_ref = actor.process_and_filter.remote(self.data)
                        dataset = ray.get(result_ref)  # åŒæ­¥è·å–ç»“æœ

                        # å¤„ç†è¾“å‡º
                        if op.stats_export_path:
                            dataset.write_json(op.stats_export_path, force_ascii=False)
                        self.data = dataset

                    except ray.exceptions.RayActorError as e:
                        print(f"Actor failed, falling back to CPU: {e}")
                        # CPUå›é€€é€»è¾‘
                        self.data = self.data.map_batches(
                            partial(filter_batch, filter_func=op.process),
                            batch_size=batch_size,
                            batch_format="pyarrow"
                        )
                else:
                    # CPUæ¨¡å¼ç›´æ¥è°ƒç”¨ï¼ˆæ— éœ€Actorï¼‰
                    if op.stats_export_path is not None:
                        self.data = self.data.map_batches(
                            op.compute_stats,
                            batch_size=batch_size,
                            batch_format="pyarrow"
                        ).write_json(op.stats_export_path, force_ascii=False)
                    
                    if op.is_batched_op():
                        self.data = self.data.map_batches(
                            partial(filter_batch, filter_func=op.process),
                            batch_size=batch_size,
                            batch_format="pyarrow",
                            zero_copy_batch=True
                        )
                    
            elif isinstance(op, Deduplicator):
                self.data = op.run(self.data)
            else:
                logger.error(
                    'Ray executor only support Filter and Mapper OPs for now')
                raise NotImplementedError
                
        except Exception as e:
            logger.error(f'An error occurred during Op [{op._name}]: {e}')
            import traceback
            traceback.print_exc()
            exit(1)
    @classmethod
    def read(cls, data_format: str, paths: Union[str, List[str]]) -> RayDataset:
        if data_format in {"json", "jsonl"}:
            return RayDataset.read_json(paths)
        elif data_format == "webdataset":
            return RayDataset.read_webdataset(paths)
        elif data_format in {
            "parquet",
            "images",
            "parquet_bulk",
            "csv",
            "text",
            "avro",
            "numpy",
            "tfrecords",
            "binary_files",
            "lance",
        }:
            return getattr(ray.data, f"read_{data_format}")(paths)

    @classmethod
    def read_json(cls, paths: Union[str, List[str]]) -> RayDataset:
        # Note: a temp solution for reading json stream
        # TODO: replace with ray.data.read_json_stream once it is available
        import pyarrow.json as js

        try:
            js.open_json
            return read_json_stream(paths)
        except AttributeError:
            return ray.data.read_json(paths)

    @classmethod
    def read_webdataset(cls, paths: Union[str, List[str]]) -> RayDataset:
        return ray.data.read_webdataset(paths, decoder=partial(_custom_default_decoder, format="PIL"))

    def to_list(self) -> list:
        return self.data.to_pandas().to_dict(orient="records")


class JSONStreamDatasource(ray.data.read_api.JSONDatasource):
    """
    A temp Datasource for reading json stream.

    Note:

        Depends on a customized `pyarrow` with `open_json` method.
    """

    def _read_stream(self, f: "pyarrow.NativeFile", path: str):
        from pyarrow.json import open_json

        try:
            reader = open_json(
                f,
                read_options=self.read_options,
                **self.arrow_json_args,
            )
            schema = None
            while True:
                try:
                    batch = reader.read_next_batch()
                    table = pyarrow.Table.from_batches([batch], schema=schema)
                    if schema is None:
                        schema = table.schema
                    yield table
                except StopIteration:
                    return
        except pyarrow.lib.ArrowInvalid as e:
            raise ValueError(f"Failed to read JSON file: {path}.") from e


def read_json_stream(
    paths: Union[str, List[str]],
    *,
    filesystem: Optional["pyarrow.fs.FileSystem"] = None,
    parallelism: int = -1,
    ray_remote_args: Dict[str, Any] = None,
    arrow_open_stream_args: Optional[Dict[str, Any]] = None,
    meta_provider=None,
    partition_filter=None,
    partitioning=ray.data.read_api.Partitioning("hive"),
    include_paths: bool = False,
    ignore_missing_paths: bool = False,
    shuffle: Union[Literal["files"], None] = None,
    file_extensions: Optional[List[str]] = ["json", "jsonl"],
    concurrency: Optional[int] = None,
    override_num_blocks: Optional[int] = None,
    **arrow_json_args,
) -> ray.data.Dataset:
    if meta_provider is None:
        meta_provider = ray.data.read_api.DefaultFileMetadataProvider()

    datasource = JSONStreamDatasource(
        paths,
        arrow_json_args=arrow_json_args,
        filesystem=filesystem,
        open_stream_args=arrow_open_stream_args,
        meta_provider=meta_provider,
        partition_filter=partition_filter,
        partitioning=partitioning,
        ignore_missing_paths=ignore_missing_paths,
        shuffle=shuffle,
        include_paths=include_paths,
        file_extensions=file_extensions,
    )
    return ray.data.read_datasource(
        datasource,
        parallelism=parallelism,
        ray_remote_args=ray_remote_args,
        concurrency=concurrency,
        override_num_blocks=override_num_blocks,
    )